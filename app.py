# app.py
import streamlit as st
import pandas as pd
import torch
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# åŒ¯å…¥æ‚¨å°ˆæ¡ˆä¸­æ—¢æœ‰çš„å‡½å¼
# ç¢ºä¿é€™äº›æª”æ¡ˆèˆ‡ app.py åœ¨åŒä¸€å€‹ Python ç’°å¢ƒä¸­å¯ä»¥è¢«æ‰¾åˆ°
from FlowLSTM import FlowLSTM
from src.plot_chart import plot_prediction_vs_observation
from src.ModelEV import ModelTest
from src.utils import create_dataset, inverse_transform_flow

# --- å°‡ pred1.py çš„æ ¸å¿ƒå‡½å¼è¤‡è£½æˆ–åŒ¯å…¥åˆ°é€™è£¡ ---

# ä½¿ç”¨ Streamlit å¿«å–ä¾†é¿å…é‡è¤‡è¼‰å…¥æ¨¡å‹ï¼Œæå‡æ•ˆèƒ½
@st.cache_resource
def load_model(model_path):
    """
    å¾æŒ‡å®šè·¯å¾‘åŠ è¼‰å·²è¨“ç·´å¥½çš„ PyTorch æ¨¡å‹ã€MinMaxScaler å’Œæ¨¡å‹åƒæ•¸ã€‚
    """
    st.info(f"æ­£åœ¨å¾ {model_path} è¼‰å…¥æ¨¡å‹...")
    # æ³¨æ„weights_only=Falseï¼Œå› ç‚ºæˆ‘å€‘éœ€è¦è¼‰å…¥ scaler çš„ numpy é™£åˆ—
    checkpoint = torch.load(model_path, weights_only=False, map_location=torch.device('cpu'))

    model_params = checkpoint.get('model_params', {})
    model = FlowLSTM(
        input_size=model_params.get('input_size', 2),
        hidden_size=model_params.get('hidden_size', 64),
        num_layers=model_params.get('num_layers', 2),
        output_size=model_params.get('output_size', 1)
    )

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # é‡å»ºscaler
    scaler = MinMaxScaler()
    scaler.min_ = checkpoint['scaler_min']
    scaler.scale_ = checkpoint['scaler_scale']
    
    st.success("æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
    return model, scaler, model_params.get('lookback'), model_params.get('forecast_horizon')

def predict_future_flow(model, scaler, last_observed_data):
    """
    å°å–®ä¸€ lookback çª—å£çš„æ•¸æ“šé€²è¡Œä¸€æ¬¡æµé‡é æ¸¬ã€‚
    """
    scaled_input = scaler.transform(last_observed_data)
    device = next(model.parameters()).device
    input_tensor = torch.tensor(scaled_input, dtype=torch.float32).unsqueeze(0).to(device)
    with torch.no_grad():
        scaled_predictions = model(input_tensor).cpu().numpy()[0]
    prediction = inverse_transform_flow(scaled_predictions, scaler)
    return prediction

def load_and_prepare_data(uploaded_file, shift_hours):
    """
    å¾ä¸Šå‚³çš„æª”æ¡ˆåŠ è¼‰ä¸¦é è™•ç†æ•¸æ“šã€‚åŒ…å«ç·šæ€§å…§æ’åŠŸèƒ½ä¾†å¡«è£œç¼ºæ¼å€¼ã€‚
    """
    if uploaded_file is None:
        return None
        
    df = pd.read_csv(uploaded_file)
    # ç¢ºä¿ 'obstime' æ¬„ä½å­˜åœ¨
    if 'obstime' not in df.columns:
        st.error("ä¸Šå‚³çš„ CSV æª”æ¡ˆç¼ºå°‘ 'obstime' æ¬„ä½ï¼Œç„¡æ³•ç¹¼çºŒè™•ç†ã€‚")
        return None

    df['date_column'] = pd.to_datetime(df['obstime'])
    df = df.set_index('date_column')
    
    # --- æ–°å¢ï¼šè³‡æ–™è£œéºåŠŸèƒ½ ---
    # 1. å°‡ä»£è¡¨ç¼ºæ¼çš„è² å€¼ï¼ˆå¦‚ -1ï¼‰æ›¿æ›ç‚º NaNï¼Œä»¥ä¾¿é€²è¡Œå…§æ’
    st.write("---")
    st.write("ğŸ” **è³‡æ–™è£œéºæª¢æŸ¥:**")
    for col in ['rainfall', 'flow']:
        if col in df.columns:
            missing_count = (df[col] < 0).sum()
            if missing_count > 0:
                df[col] = df[col].apply(lambda x: np.nan if x < 0 else x)
                st.info(f"åœ¨ '{col}' æ¬„ä½ä¸­ç™¼ç¾ {missing_count} ç­†ç¼ºæ¼å€¼ï¼Œå°‡é€²è¡Œç·šæ€§å…§æ’è™•ç†ã€‚")
    
    # 2. åŸ·è¡Œç·šæ€§å…§æ’
    df.interpolate(method='linear', inplace=True)

    # è£½ä½œé æ¸¬é›¨é‡ç‰¹å¾µ
    df[f'fcstrain_{shift_hours}h'] = df['rainfall'].shift(periods=-shift_hours, fill_value=0)
    return df

def perform_iterative_forecast(df, model, scaler, X, lookback, forecast_horizon, lead_hours):
    """
    åŸ·è¡Œè¿­ä»£å¼ï¼ˆæ»¾å‹•ï¼‰é æ¸¬ã€‚
    """
    all_predictions = []
    initial_times = []
    
    progress_bar = st.progress(0, text="æ­£åœ¨åŸ·è¡Œè¿­ä»£é æ¸¬...")

    for index, initial_input in enumerate(X):
        if lookback + index + lead_hours > len(df):
            st.warning(f"æ•¸æ“šé•·åº¦ä¸è¶³ï¼Œæ–¼ç´¢å¼• {index} åœæ­¢é æ¸¬ã€‚")
            break

        current_input = initial_input.copy()
        step_predictions = []

        for i in range(0, lead_hours, forecast_horizon):
            pred_values = predict_future_flow(model, scaler, current_input)
            num_preds = len(pred_values)
            step_predictions.extend(pred_values)
            current_input = np.roll(current_input, -num_preds, axis=0)
            for j in range(num_preds):
                next_rainfall_index = lookback + index + i + j
                next_rainfall = df['fcstrain_1h'].iloc[next_rainfall_index]
                current_input[-(num_preds-j)] = [pred_values[j], next_rainfall]

        all_predictions.append(step_predictions[:lead_hours])
        initial_time = df.index[lookback + index]
        initial_times.append(initial_time)
        
        # æ›´æ–°é€²åº¦æ¢
        progress_bar.progress((index + 1) / len(X), text=f"æ­£åœ¨é æ¸¬èµ·å§‹æ™‚é–“: {initial_time.strftime('%Y-%m-%d %H:%M')}")

    progress_bar.empty() # å®Œæˆå¾Œç§»é™¤é€²åº¦æ¢
    
    pred_columns = [f'h{i+1}' for i in range(min(lead_hours, len(all_predictions[0])))]
    df_pred = pd.DataFrame(all_predictions, columns=pred_columns, index=initial_times)
    return df_pred

# --- Streamlit App ä¸»é«” ---

st.set_page_config(page_title="é™é›¨é€•æµé æ¸¬ç³»çµ±", layout="wide")

st.title("ğŸŒŠ LSTM é™é›¨é€•æµé æ¸¬æ‡‰ç”¨")
st.markdown("""
é€™æ˜¯ä¸€å€‹ä½¿ç”¨ LSTM æ¨¡å‹é€²è¡Œé™é›¨é€•æµé æ¸¬çš„å±•ç¤ºæ‡‰ç”¨ã€‚
è«‹åœ¨å·¦å´å´é‚Šæ¬„ä¸Šå‚³æ‚¨çš„ CSV è³‡æ–™æª”æ¡ˆï¼Œç„¶å¾Œé»æ“ŠæŒ‰éˆ•é–‹å§‹é æ¸¬ã€‚
""")

# --- å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ åƒæ•¸è¨­å®š")
    
    # æª”æ¡ˆä¸Šå‚³
    uploaded_file = st.file_uploader(
        "è«‹ä¸Šå‚³ CSV æ ¼å¼çš„é æ¸¬è³‡æ–™",
        type="csv",
        help="CSV æª”æ¡ˆå¿…é ˆåŒ…å« 'obstime', 'rainfall', 'flow' ä¸‰å€‹æ¬„ä½ã€‚"
    )
    
    # æ–°å¢ï¼šç¯„ä¾‹å±•ç¤ºæŒ‰éˆ•
    if st.button("è¼‰å…¥ç¯„ä¾‹è³‡æ–™"):
        st.session_state.run_demo = True
        st.session_state.uploaded_file = None # æ¸…é™¤å·²ä¸Šå‚³çš„æª”æ¡ˆç‹€æ…‹
        # ä½¿ç”¨ st.rerun() ä¾†ç«‹å³åˆ·æ–°æ‡‰ç”¨ç‹€æ…‹
        st.rerun()

    # å¦‚æœä½¿ç”¨è€…ä¸Šå‚³æ–°æª”æ¡ˆï¼Œå‰‡å–æ¶ˆç¯„ä¾‹æ¨¡å¼
    if uploaded_file is not None:
        st.session_state.run_demo = False

    # é æ¸¬åƒæ•¸
    LEAD_HOURS_TO_FORECAST = st.slider("é æ¸¬æœªä¾†æ™‚æ•¸ (Lead Hours)", 1, 24, 6)
    LEAD_TIMES_TO_EVALUATE = [1, 3, 6]
    MODEL_PATH = 'flow_prediction_model.pth'

# --- ä¸»é é¢é‚è¼¯ ---
if uploaded_file is not None:
    st.session_state.run_demo = False
    data_source = uploaded_file
elif 'run_demo' in st.session_state and st.session_state.run_demo:
    st.info("æ‚¨æ­£åœ¨ä½¿ç”¨ç¯„ä¾‹è³‡æ–™é€²è¡Œå±•ç¤ºã€‚")
    data_source = 'data/rainfall_flow_aligned.csv'
else:
    data_source = None

if data_source is not None:
    # è¼‰å…¥ä¸¦æº–å‚™è³‡æ–™
    # é€™æ¬¡å…ˆè¼‰å…¥å®Œæ•´è³‡æ–™ä»¥å–å¾—æ—¥æœŸç¯„åœ
    full_df = load_and_prepare_data(data_source, shift_hours=1)

    if full_df is not None:
        with st.sidebar:
            st.markdown("---")
            st.header("ğŸ—“ï¸ æ—¥æœŸç¯„åœç¯©é¸")
            min_date = full_df.index.min().date()
            max_date = full_df.index.max().date()

            # å¦‚æœæ˜¯ç¯„ä¾‹æ¨¡å¼ï¼Œè¨­å®šé è¨­æ—¥æœŸ
            if 'run_demo' in st.session_state and st.session_state.run_demo:
                start_date_default = pd.to_datetime("2022-09-01").date()
                end_date_default = pd.to_datetime("2022-09-30").date()
            else:
                start_date_default = min_date
                end_date_default = max_date

            start_date = st.date_input("é–‹å§‹æ—¥æœŸ", start_date_default, min_value=min_date, max_value=max_date)
            end_date = st.date_input("çµæŸæ—¥æœŸ", end_date_default, min_value=start_date, max_value=max_date)

    # è¼‰å…¥æ¨¡å‹ (æœƒè¢«å¿«å–)
    model, scaler, lookback, forecast_horizon = load_model(MODEL_PATH)
    
    # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
    st.sidebar.info(f"""
    **æ¨¡å‹è³‡è¨Š:**
    - **Lookback:** `{lookback}` å°æ™‚
    - **Forecast Horizon:** `{forecast_horizon}` å°æ™‚
    """)

    # æ ¹æ“šé¸æ“‡çš„æ—¥æœŸç¯„åœç¯©é¸è³‡æ–™ï¼Œç”¨æ–¼é¡¯ç¤ºé è¦½
    start_datetime_preview = pd.to_datetime(start_date)
    end_datetime_preview = pd.to_datetime(end_date) + pd.Timedelta(days=1, seconds=-1)
    df_preview = full_df.loc[start_datetime_preview:end_datetime_preview]

    st.subheader(f"è³‡æ–™é è¦½ (å¾ {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')})")
    st.dataframe(df_preview.head())

    # åŸ·è¡Œé æ¸¬æŒ‰éˆ•
    if st.button("ğŸš€ åŸ·è¡Œé æ¸¬", type="primary"):
        if df_preview.empty:
            st.error("é¸å®šçš„æ—¥æœŸç¯„åœå…§æ²’æœ‰è³‡æ–™ï¼Œè«‹é‡æ–°é¸æ“‡ã€‚")
        else:
            with st.spinner("æ­£åœ¨æº–å‚™è³‡æ–™ä¸¦åŸ·è¡Œé æ¸¬ï¼Œè«‹ç¨å€™..."):
                # æº–å‚™é æ¸¬æ•¸æ“šé›†
                # **é‡è¦ï¼šä½¿ç”¨å®Œæ•´çš„ full_df ä¾†å‰µå»º Xï¼Œä»¥ç¢ºä¿æœ‰è¶³å¤ çš„æœªä¾†æ•¸æ“š**
                FEATURE_COLS = ['flow', 'fcstrain_1h']
                raw_data_values = full_df[FEATURE_COLS].values
                X, y = create_dataset(raw_data_values, lookback, forecast_horizon)

                # åŸ·è¡Œè¿­ä»£é æ¸¬
                df_pred_full = perform_iterative_forecast(full_df, model, scaler, X, lookback, forecast_horizon, LEAD_HOURS_TO_FORECAST)

                st.subheader("ğŸ“Š é æ¸¬çµæœ")
                # å¾å®Œæ•´çš„é æ¸¬çµæœä¸­ï¼Œç¯©é¸å‡ºä½¿ç”¨è€…æƒ³çœ‹çš„æ—¥æœŸç¯„åœ
                df_pred = df_pred_full.loc[start_datetime_preview:end_datetime_preview]
                st.dataframe(df_pred)

                st.subheader("ğŸ“ˆ è©•ä¼°èˆ‡è¦–è¦ºåŒ–")
                
                # é‡å°è¦è©•ä¼°çš„ lead time é€²è¡Œè¿­ä»£
                for pred_hour in LEAD_TIMES_TO_EVALUATE:
                    if f'h{pred_hour}' not in df_pred.columns:
                        continue

                    st.markdown(f"--- \n ### é ˜å…ˆæ™‚é–“ (Lead Time): {pred_hour} å°æ™‚")
                    
                    # æº–å‚™è§€æ¸¬èˆ‡é æ¸¬æ•¸æ“šä»¥é€²è¡Œæ¯”å°
                    pred_series = df_pred[f'h{pred_hour}'].copy()
                    pred_series.index += pd.to_timedelta(pred_hour, unit='h')
                    pred_series.name = 'pred_flow'
                    obs_series = full_df[['flow', 'rainfall']].copy() # ä½¿ç”¨ full_df ä¾†åŒ¹é…è§€æ¸¬å€¼
                    eval_df = pd.merge(obs_series, pred_series, left_index=True, right_index=True, how='inner')

                    if eval_df.empty:
                        st.warning(f"ç„¡æ³•å°é½Š h{pred_hour} çš„æ•¸æ“šï¼Œè·³éè©•ä¼°ã€‚")
                        continue

                    # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
                    obs_flow = eval_df['flow'].to_numpy()
                    pred_flow = eval_df['pred_flow'].to_numpy()
                    model_eva = ModelTest(qo=obs_flow, preq=pred_flow)

                    # åœ¨ç¶²é ä¸Šé¡¯ç¤ºè©•ä¼°æŒ‡æ¨™
                    col1, col2, col3 = st.columns(3)
                    col1.metric("æ•ˆç‡ä¿‚æ•¸ (CE)", f"{model_eva.get('CE', 0):.3f}")
                    col2.metric("ç›¸é—œä¿‚æ•¸ (COR)", f"{model_eva.get('COR', 0):.3f}")
                    col3.metric("æ´ªå³°èª¤å·® (EQP %)", f"{model_eva.get('EQP', 0):.2f}%")

                    # ç¹ªè£½åœ–è¡¨
                    fig = plot_prediction_vs_observation(
                        obs_df=eval_df[['flow', 'rainfall']],
                        pred_df=eval_df[['pred_flow']],
                        pred_hour=pred_hour,
                        metrics=model_eva
                    )
                    if fig:
                        st.pyplot(fig)

else:
    st.info("è«‹å¾å·¦å´å´é‚Šæ¬„ä¸Šå‚³è³‡æ–™æª”æ¡ˆä»¥é–‹å§‹ã€‚")
